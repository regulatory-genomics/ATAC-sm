
##### global workflow dependencies #####
conda: "envs/global.yaml"
# ===================================================
#                    Setup Section
# ===================================================
# libraries
import os
import sys
import shutil

import pandas as pd
import yaml
from peppy import Project
from snakemake.utils import validate, min_version
from string import Template

##### set minimum snakemake version #####
min_version("8.20.1")

module_name = "atacseq_pipeline"
report: os.path.join("report", "workflow.rst")
# list of names of the used environment specifications in workflow/envs/{env_name}.yaml
envs = ["macs2_homer","multiqc","pybedtools","uropa","ggplot","ataqv"]

# load config and sample annotation sheets #####
configfile: os.path.join("config","config_test.yaml")
validate(config, schema="../schemas/config.schema.yml")


include: os.path.join("rules", "common.smk")

# Load PEP project to get sample metadata
pep_config_value = config["project"].get("pep_config", os.path.join("..","pep", "project_config.yaml"))
pep_config_path = pep_config_value if os.path.isabs(pep_config_value) else os.path.join(workflow.basedir, pep_config_value)

# Load PEP project
pep_project = Project(pep_config_path)

# Merge PEP path variables into config for easier access
# This allows config.yaml to use shorter path references via config["paths"]
if "paths" in pep_project.config:
    if "paths" not in config:
        config["paths"] = {}
    config["paths"].update(pep_project.config["paths"])

# Helper function to resolve path placeholders in config
# Replaces {database_dir}, {shared_genome_dir}, etc. with actual paths from PEP
def resolve_config_paths(cfg, paths_dict):
    """Recursively resolve path placeholders in config dictionary"""
    if isinstance(cfg, dict):
        return {k: resolve_config_paths(v, paths_dict) for k, v in cfg.items()}
    elif isinstance(cfg, list):
        return [resolve_config_paths(item, paths_dict) for item in cfg]
    elif isinstance(cfg, str) and "{" in cfg and "}" in cfg:
        # Replace placeholders like {database_dir}/hg38/file.fa
        for key, value in paths_dict.items():
            cfg = cfg.replace(f"{{{key}}}", value)
        return cfg
    else:
        return cfg

# Resolve path placeholders in config using PEP paths
if "paths" in config:
    config = resolve_config_paths(config, config["paths"])

# Get sample table from PEP (PEP handles derive modifiers like raw_data| prefix expansion)
annot = pep_project.sample_table.copy()

# PEP may auto-merge duplicate sample_name rows, creating list-valued columns.
# We need to explode these back into separate rows if they exist.
# Check for list-valued columns and explode them
list_cols = [col for col in annot.columns if annot[col].apply(lambda x: isinstance(x, list)).any()]
if list_cols:
    # Explode list-valued columns to separate rows
    for col in list_cols:
        annot = annot.explode(col, ignore_index=True)

# Remove duplicate (sample_name, run) pairs if they exist (keep first occurrence)
# This handles cases where PEP or the CSV file might have duplicates
if 'sample_name' in annot.columns and 'run' in annot.columns:
    annot = annot.drop_duplicates(subset=['sample_name', 'run'], keep='first').reset_index(drop=True)

# Ensure required columns exist
required_columns = {"sample_name", "run"}
missing_required = required_columns.difference(annot.columns)
if missing_required:
    raise ValueError(f"Sample table missing required columns: {', '.join(sorted(missing_required))}.")

# Ensure read_type is present
if 'read_type' not in annot.columns:
    annot['read_type'] = config["project"]["read_type"]

# Ensure run column is integer before validation
annot['run'] = pd.to_numeric(annot['run'], errors='raise').astype(int)

# Ensure passqc column is integer if it exists (before validation)
if 'passqc' in annot.columns:
    annot['passqc'] = pd.to_numeric(annot['passqc'], errors='coerce').fillna(0).astype(int)

# Create a unique identifier for each run: sample_name_run
annot['sample_name'] = annot['sample_name'].astype(str)
annot['sample_run'] = annot['sample_name'] + '_' + annot['run'].astype(str)

# Validate sample table
validate(annot, schema="../schemas/samples.schema.yml")

# Check for duplicate sample_run identifiers
duplicates = annot[annot.duplicated(subset=['sample_run'], keep=False)]
if not duplicates.empty:
    sys.stderr.write("\nError: Duplicate (sample_name + run) pairs found in sample sheet!\n")
    sys.stderr.write("The following rows are not unique:\n")
    sys.stderr.write(str(duplicates[['sample_name', 'run', 'sample_run']]) + "\n\n")
    sys.exit(1)

# Set sample_run as index
annot = annot.set_index('sample_run')

# Get unique sample names (for downstream analysis)
samples = annot.reset_index().drop_duplicates(subset='sample_name', keep='first').set_index("sample_name").to_dict(orient="index")

# Get replicate sample groups (for reproducibility analysis), but only keep those with >1 sample (as determined by get_reproducibility_sample)
if 'replicate_sample_name' in annot.columns:
    all_replicate_names = (
        annot['replicate_sample_name']
        .dropna()
        .astype(str)
        .unique()
        .tolist()
    )
    # Only keep replicate names with >1 associated sample
    replicate_samples = [rep for rep in all_replicate_names if len(get_reproducibility_sample(rep)) > 1]
else:
    replicate_samples = []

# Get the original annotation sheet path from PEP for export/reporting
sample_table_entry = pep_project.config.get("sample_table")
if sample_table_entry is None:
    raise ValueError(f"PEP config {pep_config_path} missing 'sample_table'.")
annotation_sheet_path = sample_table_entry if os.path.isabs(sample_table_entry) else os.path.abspath(
    os.path.join(os.path.dirname(pep_config_path), sample_table_entry)
)

##### set global variables
result_path = config["project"]["out_dir"]
HOMER_path = os.path.abspath(os.path.join("resources", config["project"]["name"], "HOMER"))

# to deal with rule ambiguity concerning final outputs in counts/
ruleorder: sample_annotation > quantify_aggregate

# ===================================================
#                    Rules Section
# ===================================================
rule all:
    input:
        # PROCESSING
        multiqc_report = os.path.join(result_path,"report","multiqc_report.html"),
        # QUANTIFICATION
        sample_annotation = os.path.join(result_path, "downstream_res", "annotation", "sample_annotation.csv"),
        sample_annotation_plot = os.path.join(result_path, "report", "sample_annotation.png"),
        support_counts = os.path.join(result_path,"downstream_res","quantification","support_counts.csv"),
        consensus_counts = os.path.join(result_path,"downstream_res","quantification","consensus_counts.csv"),
        promoter_counts = os.path.join(result_path,"downstream_res","quantification","promoter_counts.csv"),
        tss_counts = os.path.join(result_path,"downstream_res","quantification","TSS_counts.csv"),
        HOMER_knownMotifs = os.path.join(result_path,"downstream_res","quantification","HOMER_knownMotifs.csv"),
        merged_peaks_count_matrix = os.path.join(result_path,"downstream_res","quantification","merged_peaks_count_matrix.txt"),
        # ANNOTATION
        consensus_annotation = os.path.join(result_path,'downstream_res','annotation',"consensus_annotation.csv"),
        # QC - ATAQV
        ataqv_report = os.path.join(result_path, "report", "ataqv_report"),
        # ALIGNMENT STATS REPORT
        align_stats_report = os.path.join(result_path, "report", "align_stats_report.tsv"),
        # EXPORT environments and configurations
        envs = expand(os.path.join(result_path,'envs','{env}.yaml'),env=envs),
        configs = os.path.join(result_path,'configs','{}_config.yaml'.format(config["project"]["name"])),
        annotations = os.path.join(result_path,'configs','{}_annot.csv'.format(config["project"]["name"])),
        # GLOBAL REPRODUCIBILITY (deeptools), only if replicate_sample_name is defined
        reproduciblity_result = expand(
            os.path.join(result_path, "report", "bamReproducibility", "{sample_rep}_global_rep_cor.txt"),
            sample_rep=replicate_samples
        ) if len(replicate_samples) > 0 else [],
        # REPLICATE PSEUDO-REPLICATE PEAKS (IDR/overlap), only if replicate_sample_name is defined
        replicate_peaks_pr = expand(
            os.path.join(result_path, "important_processed", "replicates", "{replicate}-pr1_vs_{replicate}-pr2.narrowPeak.gz"),
            replicate=replicate_samples
        ) if len(replicate_samples) > 0 else [],
        merged_peaks = os.path.join(result_path,"downstream_res","merged_peaks","merged_peaks.bed"),
        tracks_out = expand(os.path.join(result_path, "important_processed", "tracks", "{sample}.bw"), sample=list(samples.keys())),
    resources:
        mem_mb=config["resources"]["mem_mb"],
    threads: config["resources"]["threads"]
    log:
        os.path.join("logs","rules","all.log")

##### load rules #####
include: os.path.join("rules", "export.smk")
include: os.path.join("rules", "resources.smk")
include: os.path.join("rules", "genome_prepare.smk")
include: os.path.join("rules", "processing.smk")
include: os.path.join("rules", "report.smk")
include: os.path.join("rules", "quantification.smk")
include: os.path.join("rules", "region_annotation.smk")
include: os.path.join("rules", "qc.smk")
include: os.path.join("rules", "replicates.smk")
